# ğŸ“š HÆ°á»›ng Dáº«n Há»c Redis vÃ  Kafka Tá»« CÆ¡ Báº£n Äáº¿n NÃ¢ng Cao

> TÃ i liá»‡u nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ cho ngÆ°á»i má»›i báº¯t Ä‘áº§u, giáº£i thÃ­ch dá»… hiá»ƒu vá»›i vÃ­ dá»¥ thá»±c táº¿.

---

## ğŸ“– Má»¥c Lá»¥c

1. [Pháº§n 1: Redis](#pháº§n-1-redis)
   - [1.1. Redis lÃ  gÃ¬?](#11-redis-lÃ -gÃ¬)
   - [1.2. Táº¡i sao cáº§n Redis?](#12-táº¡i-sao-cáº§n-redis)
   - [1.3. Kiáº¿n trÃºc Redis](#13-kiáº¿n-trÃºc-redis)
   - [1.4. CÃ¡c kiá»ƒu dá»¯ liá»‡u trong Redis](#14-cÃ¡c-kiá»ƒu-dá»¯-liá»‡u-trong-redis)
   - [1.5. CÃ¡c use case thá»±c táº¿](#15-cÃ¡c-use-case-thá»±c-táº¿)
   - [1.6. Redis nÃ¢ng cao](#16-redis-nÃ¢ng-cao)

2. [Pháº§n 2: Kafka](#pháº§n-2-kafka)
   - [2.1. Kafka lÃ  gÃ¬?](#21-kafka-lÃ -gÃ¬)
   - [2.2. Táº¡i sao cáº§n Kafka?](#22-táº¡i-sao-cáº§n-kafka)
   - [2.3. Kiáº¿n trÃºc Kafka](#23-kiáº¿n-trÃºc-kafka)
   - [2.4. CÃ¡c khÃ¡i niá»‡m cá»‘t lÃµi](#24-cÃ¡c-khÃ¡i-niá»‡m-cá»‘t-lÃµi)
   - [2.5. CÃ¡c use case thá»±c táº¿](#25-cÃ¡c-use-case-thá»±c-táº¿)
   - [2.6. Kafka nÃ¢ng cao](#26-kafka-nÃ¢ng-cao)

3. [Pháº§n 3: á»¨ng Dá»¥ng Thá»±c Táº¿](#pháº§n-3-á»©ng-dá»¥ng-thá»±c-táº¿)
   - [3.1. Kiáº¿n trÃºc á»©ng dá»¥ng demo](#31-kiáº¿n-trÃºc-á»©ng-dá»¥ng-demo)
   - [3.2. So sÃ¡nh hiá»‡u suáº¥t](#32-so-sÃ¡nh-hiá»‡u-suáº¥t)

4. [Pháº§n 4: Best Practices](#pháº§n-4-best-practices)

---

# Pháº§n 1: Redis

## 1.1. Redis lÃ  gÃ¬?

**Redis** (Remote Dictionary Server) lÃ  má»™t **cÆ¡ sá»Ÿ dá»¯ liá»‡u lÆ°u trá»¯ trÃªn bá»™ nhá»›** (in-memory database) dáº¡ng **key-value**.

### ğŸ¯ VÃ­ dá»¥ Ä‘Æ¡n giáº£n Ä‘á»ƒ hiá»ƒu:

TÆ°á»Ÿng tÆ°á»£ng báº¡n cÃ³ má»™t cuá»‘n sá»• tay:
- **CÆ¡ sá»Ÿ dá»¯ liá»‡u thÃ´ng thÆ°á»ng (MySQL, PostgreSQL)**: NhÆ° má»™t thÆ° viá»‡n lá»›n, báº¡n pháº£i tÃ¬m kiáº¿m sÃ¡ch trong ká»‡ (Ä‘á»c tá»« á»• cá»©ng - cháº­m)
- **Redis**: NhÆ° giáº¥y note dÃ¡n trÃªn bÃ n lÃ m viá»‡c, báº¡n nhÃ¬n lÃ  tháº¥y ngay (Ä‘á»c tá»« RAM - cá»±c nhanh)

### âš¡ Äáº·c Ä‘iá»ƒm chÃ­nh:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RAM (Redis)                            â”‚
â”‚  â€¢ Tá»‘c Ä‘á»™: 100,000+ ops/giÃ¢y           â”‚
â”‚  â€¢ Äá»™ trá»…: < 1ms                        â”‚
â”‚  â€¢ Dá»¯ liá»‡u: Táº¡m thá»i hoáº·c lÃ¢u dÃ i       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†•ï¸ (Cháº­m hÆ¡n 1000 láº§n)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  á»” cá»©ng (MySQL, PostgreSQL)             â”‚
â”‚  â€¢ Tá»‘c Ä‘á»™: ~1,000 ops/giÃ¢y              â”‚
â”‚  â€¢ Äá»™ trá»…: 10-100ms                     â”‚
â”‚  â€¢ Dá»¯ liá»‡u: LÃ¢u dÃ i, an toÃ n            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1.2. Táº¡i sao cáº§n Redis?

### ğŸ”´ Váº¥n Ä‘á» KHÃ”NG cÃ³ Redis:

TÆ°á»Ÿng tÆ°á»£ng báº¡n cÃ³ má»™t website bÃ¡n hÃ ng:

```python
# Má»—i láº§n user xem sáº£n pháº©m, pháº£i query database
def get_product(product_id):
    # Truy váº¥n database máº¥t ~50ms
    product = db.query("SELECT * FROM products WHERE id = ?", product_id)
    return product

# 1000 users cÃ¹ng lÃºc = 1000 queries = Database quÃ¡ táº£i! ğŸ’¥
```

**Háº­u quáº£:**
- Website cháº­m
- Database bá»‹ quÃ¡ táº£i
- Chi phÃ­ server tÄƒng
- Tráº£i nghiá»‡m ngÆ°á»i dÃ¹ng tá»‡

### ğŸŸ¢ Giáº£i phÃ¡p vá»›i Redis:

```python
# Láº§n Ä‘áº§u tiÃªn, lÆ°u vÃ o Redis
def get_product(product_id):
    # Kiá»ƒm tra Redis trÆ°á»›c (~0.1ms)
    product = redis.get(f"product:{product_id}")
    
    if product:
        return product  # Tráº£ vá» ngay tá»« Redis
    
    # Náº¿u khÃ´ng cÃ³ trong Redis, má»›i query database
    product = db.query("SELECT * FROM products WHERE id = ?", product_id)
    
    # LÆ°u vÃ o Redis cho láº§n sau
    redis.set(f"product:{product_id}", product, expire=3600)  # LÆ°u 1 giá»
    
    return product

# 1000 users cÃ¹ng lÃºc:
# - User Ä‘áº§u: Query database 1 láº§n
# - 999 users cÃ²n láº¡i: Äá»c tá»« Redis (cá»±c nhanh)
# Database chá»‰ xá»­ lÃ½ 1 query thay vÃ¬ 1000! âœ¨
```

**Káº¿t quáº£:**
- Tá»‘c Ä‘á»™ nhanh hÆ¡n 100-1000 láº§n
- Database giáº£m táº£i 99%
- Tiáº¿t kiá»‡m chi phÃ­
- NgÆ°á»i dÃ¹ng hÃ i lÃ²ng

---

## 1.3. Kiáº¿n trÃºc Redis

### ğŸ—ï¸ Cáº¥u trÃºc cÆ¡ báº£n:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Application (Your Code)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Redis Client (Library)                  â”‚
â”‚   â€¢ Python: redis-py                              â”‚
â”‚   â€¢ Node.js: ioredis                              â”‚
â”‚   â€¢ Java: Jedis                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“ (TCP Connection)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Redis Server                         â”‚
â”‚                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚         RAM (Main Memory)              â”‚      â”‚
â”‚  â”‚                                         â”‚      â”‚
â”‚  â”‚  Key: "user:1001"                      â”‚      â”‚
â”‚  â”‚  Value: {"name": "John", "age": 25}    â”‚      â”‚
â”‚  â”‚                                         â”‚      â”‚
â”‚  â”‚  Key: "session:abc123"                 â”‚      â”‚
â”‚  â”‚  Value: "user_data_here"               â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                     â”‚                             â”‚
â”‚                     â†“ (Optional: Persistence)     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚         Disk (Backup)                  â”‚      â”‚
â”‚  â”‚  â€¢ RDB: Snapshot Ä‘á»‹nh ká»³               â”‚      â”‚
â”‚  â”‚  â€¢ AOF: Log má»i thao tÃ¡c                â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1.4. CÃ¡c kiá»ƒu dá»¯ liá»‡u trong Redis

Redis khÃ´ng chá»‰ lÆ°u trá»¯ string Ä‘Æ¡n giáº£n, mÃ  há»— trá»£ nhiá»u kiá»ƒu dá»¯ liá»‡u phá»©c táº¡p:

### 1ï¸âƒ£ **String** - Kiá»ƒu cÆ¡ báº£n nháº¥t

```python
# LÆ°u vÃ  láº¥y giÃ¡ trá»‹ Ä‘Æ¡n giáº£n
redis.set("name", "John")
name = redis.get("name")  # "John"

# TÄƒng/giáº£m sá»‘
redis.set("views", 0)
redis.incr("views")  # views = 1
redis.incr("views")  # views = 2
redis.decr("views")  # views = 1
```

**Use case:** Äáº¿m lÆ°á»£t view, cache dá»¯ liá»‡u Ä‘Æ¡n giáº£n, session storage

---

### 2ï¸âƒ£ **Hash** - LÆ°u object/dictionary

```python
# LÆ°u thÃ´ng tin user nhÆ° má»™t object
redis.hset("user:1001", mapping={
    "name": "John",
    "age": "25",
    "email": "john@example.com"
})

# Láº¥y má»™t field
name = redis.hget("user:1001", "name")  # "John"

# Láº¥y táº¥t cáº£
user = redis.hgetall("user:1001")
# {"name": "John", "age": "25", "email": "john@example.com"}

# Chá»‰ tÄƒng tuá»•i
redis.hincrby("user:1001", "age", 1)  # age = 26
```

**Use case:** LÆ°u thÃ´ng tin user, product details, cache object phá»©c táº¡p

---

### 3ï¸âƒ£ **List** - Danh sÃ¡ch cÃ³ thá»© tá»±

```python
# ThÃªm vÃ o cuá»‘i list (nhÆ° queue)
redis.rpush("notifications", "Message 1")
redis.rpush("notifications", "Message 2")
redis.rpush("notifications", "Message 3")

# Láº¥y tá»« Ä‘áº§u list
message = redis.lpop("notifications")  # "Message 1"

# Xem toÃ n bá»™ list (khÃ´ng xÃ³a)
all_messages = redis.lrange("notifications", 0, -1)
# ["Message 2", "Message 3"]

# Giá»¯ chá»‰ 100 items má»›i nháº¥t
redis.ltrim("recent_activities", 0, 99)
```

**Use case:** Queue (hÃ ng Ä‘á»£i), activity logs, recent items, timeline

---

### 4ï¸âƒ£ **Set** - Táº­p há»£p khÃ´ng trÃ¹ng láº·p

```python
# ThÃªm vÃ o set
redis.sadd("online_users", "user1")
redis.sadd("online_users", "user2")
redis.sadd("online_users", "user1")  # KhÃ´ng thÃªm vÃ¬ Ä‘Ã£ cÃ³

# Kiá»ƒm tra member
is_online = redis.sismember("online_users", "user1")  # True

# Láº¥y táº¥t cáº£ members
users = redis.smembers("online_users")  # {"user1", "user2"}

# Sá»‘ lÆ°á»£ng members
count = redis.scard("online_users")  # 2

# Thao tÃ¡c giá»¯a cÃ¡c sets
redis.sadd("users_group_a", "user1", "user2", "user3")
redis.sadd("users_group_b", "user2", "user3", "user4")

# Users á»Ÿ cáº£ 2 groups
common = redis.sinter("users_group_a", "users_group_b")  # {"user2", "user3"}

# Users á»Ÿ Ã­t nháº¥t 1 group
all_users = redis.sunion("users_group_a", "users_group_b")  # {"user1", "user2", "user3", "user4"}
```

**Use case:** Tags, online users, unique visitors, permissions, recommendations

---

### 5ï¸âƒ£ **Sorted Set** - Táº­p há»£p cÃ³ Ä‘iá»ƒm sá»‘

```python
# ThÃªm vá»›i Ä‘iá»ƒm sá»‘ (score)
redis.zadd("leaderboard", {"player1": 100, "player2": 200, "player3": 150})

# Top 3 (Ä‘iá»ƒm cao nháº¥t)
top3 = redis.zrevrange("leaderboard", 0, 2, withscores=True)
# [("player2", 200), ("player3", 150), ("player1", 100)]

# Thá»© háº¡ng cá»§a player
rank = redis.zrevrank("leaderboard", "player1")  # 2 (Ä‘á»©ng thá»© 3, index tá»« 0)

# TÄƒng Ä‘iá»ƒm
redis.zincrby("leaderboard", 50, "player1")  # player1 = 150

# Láº¥y players cÃ³ Ä‘iá»ƒm tá»« 100-200
players = redis.zrangebyscore("leaderboard", 100, 200)
```

**Use case:** Leaderboard, ranking, priority queue, trending posts

---

### 6ï¸âƒ£ **Pub/Sub** - Gá»­i/nháº­n message real-time

```python
# Publisher (ngÆ°á»i gá»­i)
redis.publish("news_channel", "Breaking news!")

# Subscriber (ngÆ°á»i nháº­n)
pubsub = redis.pubsub()
pubsub.subscribe("news_channel")

for message in pubsub.listen():
    if message["type"] == "message":
        print(f"Received: {message['data']}")
```

**Use case:** Chat app, notifications, live updates, broadcasting

---

## 1.5. CÃ¡c use case thá»±c táº¿

### ğŸ“Œ Use Case 1: **Caching** (Cache dá»¯ liá»‡u)

**Váº¥n Ä‘á»:** Query database cháº­m, tá»‘n tÃ i nguyÃªn

```python
# âŒ KHÃ”NG tá»‘t - Query database má»—i láº§n
def get_product_info(product_id):
    product = db.query("SELECT * FROM products WHERE id = ?", product_id)
    return product

# âœ… Tá»T - Cache báº±ng Redis
def get_product_info(product_id):
    cache_key = f"product:{product_id}"
    
    # Kiá»ƒm tra cache
    cached = redis.get(cache_key)
    if cached:
        return json.loads(cached)
    
    # KhÃ´ng cÃ³ cache, query database
    product = db.query("SELECT * FROM products WHERE id = ?", product_id)
    
    # LÆ°u vÃ o cache (expire sau 1 giá»)
    redis.setex(cache_key, 3600, json.dumps(product))
    
    return product
```

**Lá»£i Ã­ch:**
- Tá»‘c Ä‘á»™: Nhanh hÆ¡n 100-1000 láº§n
- Giáº£m táº£i database: 99% requests Ä‘á»c tá»« Redis
- Scalability: Xá»­ lÃ½ Ä‘Æ°á»£c hÃ ng triá»‡u requests

---

### ğŸ“Œ Use Case 2: **Session Management**

**Váº¥n Ä‘á»:** LÆ°u session trong memory server â†’ KhÃ´ng scale Ä‘Æ°á»£c

```python
# âœ… LÆ°u session trong Redis
from flask import Flask, session
from flask_session import Session

app = Flask(__name__)
app.config['SESSION_TYPE'] = 'redis'
app.config['SESSION_REDIS'] = redis.StrictRedis(host='localhost', port=6379)
Session(app)

@app.route('/login', methods=['POST'])
def login():
    user = authenticate(request.form['username'], request.form['password'])
    session['user_id'] = user.id
    session['username'] = user.username
    return "Logged in"

@app.route('/dashboard')
def dashboard():
    if 'user_id' in session:
        return f"Welcome {session['username']}"
    return "Please login"
```

**Lá»£i Ã­ch:**
- Session Ä‘Æ°á»£c share giá»¯a nhiá»u server
- Dá»… dÃ ng scale horizontal
- Session khÃ´ng máº¥t khi restart server

---

### ğŸ“Œ Use Case 3: **Rate Limiting** (Giá»›i háº¡n request)

**Váº¥n Ä‘á»:** NgÄƒn cháº·n spam, DDoS attack

```python
# Giá»›i háº¡n 10 requests/phÃºt cho má»—i IP
def is_rate_limited(ip_address):
    key = f"rate_limit:{ip_address}"
    
    # TÄƒng counter
    current = redis.incr(key)
    
    # Set expire láº§n Ä‘áº§u tiÃªn
    if current == 1:
        redis.expire(key, 60)  # 60 giÃ¢y
    
    # Kiá»ƒm tra giá»›i háº¡n
    if current > 10:
        return True  # VÆ°á»£t giá»›i háº¡n
    
    return False  # OK

@app.route('/api/data')
def get_data():
    ip = request.remote_addr
    
    if is_rate_limited(ip):
        return {"error": "Too many requests"}, 429
    
    return {"data": "your data"}
```

---

### ğŸ“Œ Use Case 4: **Leaderboard** (Báº£ng xáº¿p háº¡ng)

**Váº¥n Ä‘á»:** Cáº§n update vÃ  query ranking nhanh chÃ³ng

```python
# Cáº­p nháº­t Ä‘iá»ƒm
def update_score(player_id, score):
    redis.zadd("game_leaderboard", {player_id: score})

# Láº¥y top 10
def get_top_10():
    return redis.zrevrange("game_leaderboard", 0, 9, withscores=True)

# Láº¥y rank cá»§a player
def get_player_rank(player_id):
    rank = redis.zrevrank("game_leaderboard", player_id)
    return rank + 1 if rank is not None else None

# Láº¥y players xung quanh
def get_nearby_players(player_id, range=5):
    rank = redis.zrevrank("game_leaderboard", player_id)
    if rank is None:
        return []
    
    start = max(0, rank - range)
    end = rank + range
    
    return redis.zrevrange("game_leaderboard", start, end, withscores=True)
```

---

### ğŸ“Œ Use Case 5: **Real-time Analytics**

```python
# Äáº¿m page views theo thá»i gian thá»±c
def track_page_view(page_url):
    today = datetime.now().strftime("%Y-%m-%d")
    
    # TÄƒng counter cho ngÃ y hÃ´m nay
    redis.hincrby(f"pageviews:{today}", page_url, 1)
    
    # TÄƒng tá»•ng
    redis.incr(f"total_pageviews:{page_url}")

# Láº¥y thá»‘ng kÃª
def get_today_stats():
    today = datetime.now().strftime("%Y-%m-%d")
    return redis.hgetall(f"pageviews:{today}")

# Láº¥y top 10 pages
def get_trending_pages():
    today = datetime.now().strftime("%Y-%m-%d")
    pages = redis.hgetall(f"pageviews:{today}")
    
    # Sort by views
    sorted_pages = sorted(pages.items(), key=lambda x: int(x[1]), reverse=True)
    return sorted_pages[:10]
```

---

## 1.6. Redis nÃ¢ng cao

### ğŸ”„ **Persistence** (LÆ°u trá»¯ lÃ¢u dÃ i)

Redis lÆ°u dá»¯ liá»‡u trong RAM, nhÆ°ng cÃ³ thá»ƒ backup xuá»‘ng disk:

#### **1. RDB (Redis Database Backup)**
Snapshot Ä‘á»‹nh ká»³:

```bash
# redis.conf
save 900 1      # Sau 900 giÃ¢y (15 phÃºt) náº¿u cÃ³ Ã­t nháº¥t 1 thay Ä‘á»•i
save 300 10     # Sau 300 giÃ¢y (5 phÃºt) náº¿u cÃ³ Ã­t nháº¥t 10 thay Ä‘á»•i
save 60 10000   # Sau 60 giÃ¢y náº¿u cÃ³ Ã­t nháº¥t 10000 thay Ä‘á»•i
```

**Æ¯u Ä‘iá»ƒm:**
- File backup nhá» gá»n
- Restore nhanh
- Performance tá»‘t

**NhÆ°á»£c Ä‘iá»ƒm:**
- CÃ³ thá»ƒ máº¥t data giá»¯a 2 snapshot
- Snapshot lá»›n tá»‘n thá»i gian

---

#### **2. AOF (Append Only File)**
Log má»i thao tÃ¡c write:

```bash
# redis.conf
appendonly yes
appendfsync everysec  # Sync má»—i giÃ¢y (cÃ¢n báº±ng performance vÃ  safety)
```

**Æ¯u Ä‘iá»ƒm:**
- Máº¥t Ã­t data hÆ¡n (chá»‰ máº¥t tá»‘i Ä‘a 1 giÃ¢y)
- CÃ³ thá»ƒ replay láº¡i operations

**NhÆ°á»£c Ä‘iá»ƒm:**
- File lá»›n hÆ¡n RDB
- Restore cháº­m hÆ¡n

---

### ğŸ” **Transactions**

Äáº£m báº£o nhiá»u operations Ä‘Æ°á»£c thá»±c thi atomic (táº¥t cáº£ hoáº·c khÃ´ng):

```python
# Chuyá»ƒn tiá»n giá»¯a 2 tÃ i khoáº£n
pipe = redis.pipeline()

try:
    # Báº¯t Ä‘áº§u transaction
    pipe.watch("balance:user1", "balance:user2")
    
    balance1 = int(redis.get("balance:user1"))
    balance2 = int(redis.get("balance:user2"))
    
    if balance1 >= 100:  # Kiá»ƒm tra Ä‘á»§ tiá»n
        pipe.multi()
        pipe.decrby("balance:user1", 100)
        pipe.incrby("balance:user2", 100)
        pipe.execute()
        print("Transaction successful")
    else:
        print("Insufficient balance")
        
except redis.WatchError:
    print("Transaction failed - data changed by another client")
```

---

### ğŸ“Š **Redis Cluster** (PhÃ¢n tÃ¡n dá»¯ liá»‡u)

Khi dá»¯ liá»‡u quÃ¡ lá»›n cho 1 server:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Redis Cluster                       â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Master 1 â”‚  â”‚ Master 2 â”‚  â”‚ Master 3 â”‚          â”‚
â”‚  â”‚ Slots    â”‚  â”‚ Slots    â”‚  â”‚ Slots    â”‚          â”‚
â”‚  â”‚ 0-5460   â”‚  â”‚ 5461-    â”‚  â”‚ 10923-   â”‚          â”‚
â”‚  â”‚          â”‚  â”‚ 10922    â”‚  â”‚ 16383    â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â”‚
â”‚       â”‚             â”‚             â”‚                 â”‚
â”‚       â†“             â†“             â†“                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Replica 1â”‚  â”‚ Replica 2â”‚  â”‚ Replica 3â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Lá»£i Ã­ch:**
- Horizontal scaling
- High availability
- Automatic failover

---

### ğŸ”¥ **Lua Scripts** (Atomic operations phá»©c táº¡p)

Thá»±c thi logic phá»©c táº¡p atomic trÃªn Redis server:

```python
# Script Lua Ä‘á»ƒ implement rate limiting chÃ­nh xÃ¡c
lua_script = """
local key = KEYS[1]
local limit = tonumber(ARGV[1])
local window = tonumber(ARGV[2])

local current = redis.call('INCR', key)

if current == 1 then
    redis.call('EXPIRE', key, window)
end

if current > limit then
    return 0
else
    return 1
end
"""

# Register script
rate_limit = redis.register_script(lua_script)

# Sá»­ dá»¥ng
def check_rate_limit(user_id):
    result = rate_limit(keys=[f"rate:{user_id}"], args=[10, 60])
    return result == 1  # True náº¿u OK, False náº¿u vÆ°á»£t giá»›i háº¡n
```

---

# Pháº§n 2: Kafka

## 2.1. Kafka lÃ  gÃ¬?

**Apache Kafka** lÃ  má»™t **há»‡ thá»‘ng message streaming phÃ¢n tÃ¡n** (distributed streaming platform), Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ xá»­ lÃ½ vÃ  truyá»n táº£i **hÃ ng triá»‡u message má»—i giÃ¢y**.

### ğŸ¯ VÃ­ dá»¥ Ä‘Æ¡n giáº£n Ä‘á»ƒ hiá»ƒu:

TÆ°á»Ÿng tÆ°á»£ng Kafka nhÆ° má»™t **há»‡ thá»‘ng Ä‘Æ°á»ng á»‘ng nÆ°á»›c**:

```
[Nguá»“n nÆ°á»›c 1] â”€â”€â”
[Nguá»“n nÆ°á»›c 2] â”€â”€â”¤
[Nguá»“n nÆ°á»›c 3] â”€â”€â”¤â”€â”€ [ÄÆ°á»ng á»‘ng chÃ­nh (Kafka)] â”€â”€â”¬â”€â†’ [NhÃ  1]
                 â”‚                                â”œâ”€â†’ [NhÃ  2]
                 â”‚                                â””â”€â†’ [NhÃ  3]
```

- **Nguá»“n nÆ°á»›c** = Producers (á»©ng dá»¥ng gá»­i data)
- **ÄÆ°á»ng á»‘ng** = Kafka Topics (kÃªnh truyá»n táº£i)
- **NhÃ ** = Consumers (á»©ng dá»¥ng nháº­n data)

Äáº·c biá»‡t:
- NÆ°á»›c (data) Ä‘Æ°á»£c lÆ°u trong Ä‘Æ°á»ng á»‘ng má»™t thá»i gian
- Nhiá»u nhÃ  cÃ³ thá»ƒ uá»‘ng cÃ¹ng 1 nguá»“n nÆ°á»›c
- ThÃªm nhÃ  má»›i khÃ´ng áº£nh hÆ°á»Ÿng nguá»“n nÆ°á»›c

---

## 2.2. Táº¡i sao cáº§n Kafka?

### ğŸ”´ Váº¥n Ä‘á» KHÃ”NG cÃ³ Kafka:

TÆ°á»Ÿng tÆ°á»£ng báº¡n xÃ¢y dá»±ng há»‡ thá»‘ng e-commerce:

```python
# Khi user Ä‘áº·t hÃ ng, pháº£i lÃ m NHIá»€U viá»‡c:
def place_order(order_data):
    # 1. LÆ°u vÃ o database
    db.save_order(order_data)  # 50ms
    
    # 2. Gá»­i email xÃ¡c nháº­n
    send_email(order_data['email'])  # 200ms
    
    # 3. Gá»­i SMS
    send_sms(order_data['phone'])  # 300ms
    
    # 4. Update inventory
    update_inventory(order_data['items'])  # 100ms
    
    # 5. TÃ­nh Ä‘iá»ƒm thÆ°á»Ÿng
    calculate_points(order_data['user_id'])  # 50ms
    
    # 6. Gá»­i notification
    send_notification(order_data['user_id'])  # 100ms
    
    # Tá»”NG: 800ms - User pháº£i chá»!
    return "Order placed"
```

**Váº¥n Ä‘á»:**
1. **Cháº­m**: User chá» 800ms
2. **Coupling**: Táº¥t cáº£ services phá»¥ thuá»™c nhau
3. **Failure**: 1 service lá»—i â†’ ToÃ n bá»™ lá»—i
4. **KhÃ´ng scale**: KhÃ³ má»Ÿ rá»™ng

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Order   â”‚
â”‚ Service  â”‚â”€â”€â†’ Email Service (lá»—i) âŒ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â†“
                SMS Service khÃ´ng cháº¡y Ä‘Æ°á»£c
                Notification khÃ´ng cháº¡y Ä‘Æ°á»£c
                â†’ ÄÆ¡n hÃ ng tháº¥t báº¡i hoÃ n toÃ n!
```

---

### ğŸŸ¢ Giáº£i phÃ¡p vá»›i Kafka:

```python
# Producer: Chá»‰ gá»­i message vÃ o Kafka
def place_order(order_data):
    # 1. LÆ°u vÃ o database
    db.save_order(order_data)  # 50ms
    
    # 2. Gá»­i event vÃ o Kafka
    kafka.send('orders', order_data)  # 5ms
    
    # Tá»”NG: 55ms - User hÃ i lÃ²ng!
    return "Order placed"

# Consumers: Xá»­ lÃ½ Ä‘á»™c láº­p vÃ  song song
# Consumer 1: Email Service
kafka.consume('orders', lambda order: send_email(order['email']))

# Consumer 2: SMS Service
kafka.consume('orders', lambda order: send_sms(order['phone']))

# Consumer 3: Inventory Service
kafka.consume('orders', lambda order: update_inventory(order['items']))

# Consumer 4: Loyalty Service
kafka.consume('orders', lambda order: calculate_points(order['user_id']))

# Consumer 5: Notification Service
kafka.consume('orders', lambda order: send_notification(order['user_id']))
```

**Kiáº¿n trÃºc:**

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Order Service  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ (55ms)
                             â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Kafka Topic    â”‚
                    â”‚    "orders"     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â†“                â†“                â†“               â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚Email Service â”‚ â”‚ SMS Service  â”‚ â”‚Inventory Svc â”‚ â”‚Loyalty Svc   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    (Ä‘á»™c láº­p)        (Ä‘á»™c láº­p)        (Ä‘á»™c láº­p)        (Ä‘á»™c láº­p)
```

**Lá»£i Ã­ch:**
1. **Nhanh**: User chá»‰ chá» 55ms (nhanh hÆ¡n 14 láº§n)
2. **Decoupling**: Services Ä‘á»™c láº­p, khÃ´ng phá»¥ thuá»™c nhau
3. **Fault tolerance**: 1 service lá»—i khÃ´ng áº£nh hÆ°á»Ÿng khÃ¡c
4. **Scalability**: Dá»… dÃ ng thÃªm consumers má»›i
5. **Replay**: CÃ³ thá»ƒ xá»­ lÃ½ láº¡i events cÅ©

---

## 2.3. Kiáº¿n trÃºc Kafka

### ğŸ—ï¸ CÃ¡c thÃ nh pháº§n chÃ­nh:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Kafka Cluster                          â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Broker 1    â”‚  â”‚  Broker 2    â”‚  â”‚  Broker 3    â”‚      â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚      â”‚
â”‚  â”‚ Topic: logs  â”‚  â”‚ Topic: logs  â”‚  â”‚ Topic: logs  â”‚      â”‚
â”‚  â”‚ Partition 0  â”‚  â”‚ Partition 1  â”‚  â”‚ Partition 2  â”‚      â”‚
â”‚  â”‚ [msg][msg]   â”‚  â”‚ [msg][msg]   â”‚  â”‚ [msg][msg]   â”‚      â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚      â”‚
â”‚  â”‚ Topic: users â”‚  â”‚ Topic: users â”‚  â”‚ Topic: users â”‚      â”‚
â”‚  â”‚ Partition 0  â”‚  â”‚ Partition 1  â”‚  â”‚ Partition 2  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘                                      â†“
         â”‚                                      â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Producer   â”‚                         â”‚ Consumer   â”‚
    â”‚ (gá»­i data) â”‚                         â”‚ (nháº­n data)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2.4. CÃ¡c khÃ¡i niá»‡m cá»‘t lÃµi

### 1ï¸âƒ£ **Topic** (Chá»§ Ä‘á»)

Má»™t kÃªnh Ä‘á»ƒ gá»­i/nháº­n messages, giá»‘ng nhÆ° má»™t folder.

```python
# VÃ­ dá»¥ topics trong há»‡ thá»‘ng
topics = [
    "user.registered",      # User má»›i Ä‘Äƒng kÃ½
    "order.created",        # ÄÆ¡n hÃ ng má»›i
    "payment.completed",    # Thanh toÃ¡n thÃ nh cÃ´ng
    "email.sent",          # Email Ä‘Ã£ gá»­i
    "analytics.pageview"   # LÆ°á»£t xem trang
]
```

---

### 2ï¸âƒ£ **Partition** (PhÃ¢n vÃ¹ng)

Má»—i topic Ä‘Æ°á»£c chia thÃ nh nhiá»u partitions Ä‘á»ƒ:
- **Parallel processing**: Xá»­ lÃ½ song song
- **Scalability**: Má»Ÿ rá»™ng dá»… dÃ ng
- **Ordering**: Äáº£m báº£o thá»© tá»± trong partition

```
Topic: "orders"
â”œâ”€â”€ Partition 0: [msg1] [msg2] [msg3] [msg4] â†’ Consumer A
â”œâ”€â”€ Partition 1: [msg5] [msg6] [msg7] [msg8] â†’ Consumer B
â””â”€â”€ Partition 2: [msg9] [msg10][msg11][msg12] â†’ Consumer C
```

**Ordering guarantee:**
- Messages trong **cÃ¹ng partition**: Äáº£m báº£o thá»© tá»±
- Messages á»Ÿ **khÃ¡c partition**: KhÃ´ng Ä‘áº£m báº£o thá»© tá»±

```python
# Producer gá»­i vá»›i key Ä‘á»ƒ Ä‘áº£m báº£o cÃ¹ng user vÃ o cÃ¹ng partition
producer.send('orders', 
              key=str(user_id).encode(),  # Key
              value=order_data)            # Value

# CÃ¹ng user_id â†’ CÃ¹ng partition â†’ Äáº£m báº£o thá»© tá»±
```

---

### 3ï¸âƒ£ **Producer** (NgÆ°á»i gá»­i)

á»¨ng dá»¥ng gá»­i messages vÃ o Kafka.

```python
from kafka import KafkaProducer
import json

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

# Gá»­i message
order = {
    "order_id": "12345",
    "user_id": "user_001",
    "amount": 99.99,
    "items": ["item1", "item2"]
}

future = producer.send('orders', value=order)
result = future.get(timeout=10)  # Chá» confirm

print(f"Sent to partition {result.partition} at offset {result.offset}")
```

**Producer config quan trá»ng:**

```python
producer = KafkaProducer(
    # Äá»™ tin cáº­y
    acks='all',  # 0: khÃ´ng chá», 1: chá» leader, all: chá» táº¥t cáº£ replicas
    
    # Retry khi lá»—i
    retries=3,
    
    # Batch Ä‘á»ƒ tÄƒng performance
    batch_size=16384,
    linger_ms=10,  # Chá» 10ms Ä‘á»ƒ gom batch
    
    # Compression Ä‘á»ƒ giáº£m bandwidth
    compression_type='gzip'
)
```

---

### 4ï¸âƒ£ **Consumer** (NgÆ°á»i nháº­n)

á»¨ng dá»¥ng Ä‘á»c messages tá»« Kafka.

```python
from kafka import KafkaConsumer
import json

consumer = KafkaConsumer(
    'orders',  # Topic name
    bootstrap_servers=['localhost:9092'],
    value_deserializer=lambda m: json.loads(m.decode('utf-8')),
    group_id='order-processors',  # Consumer group
    auto_offset_reset='earliest'  # Äá»c tá»« Ä‘áº§u náº¿u chÆ°a cÃ³ offset
)

# Äá»c messages
for message in consumer:
    order = message.value
    print(f"Processing order {order['order_id']}")
    
    # Xá»­ lÃ½ order
    process_order(order)
    
    # Kafka tá»± Ä‘á»™ng commit offset
```

---

### 5ï¸âƒ£ **Consumer Group** (NhÃ³m consumers)

Nhiá»u consumers cÃ¹ng nhÃ³m Ä‘á»ƒ xá»­ lÃ½ song song:

```
Topic "orders" (3 partitions)
â”œâ”€â”€ Partition 0 â†’ Consumer A (group: processors)
â”œâ”€â”€ Partition 1 â†’ Consumer B (group: processors)
â””â”€â”€ Partition 2 â†’ Consumer C (group: processors)

â†’ Má»—i partition Ä‘Æ°á»£c 1 consumer xá»­ lÃ½
â†’ Load balancing tá»± Ä‘á»™ng
â†’ Fault tolerance: Consumer cháº¿t â†’ partition reassign
```

**Nhiá»u consumer groups:**

```
Topic "orders"
â”œâ”€â”€ Consumer Group "processors" (xá»­ lÃ½ Ä‘Æ¡n hÃ ng)
â”‚   â”œâ”€â”€ Consumer 1
â”‚   â””â”€â”€ Consumer 2
â”‚
â””â”€â”€ Consumer Group "analytics" (phÃ¢n tÃ­ch dá»¯ liá»‡u)
    â”œâ”€â”€ Consumer 1
    â””â”€â”€ Consumer 2

â†’ Má»—i group Ä‘á»c Ä‘á»™c láº­p toÃ n bá»™ messages
```

---

### 6ï¸âƒ£ **Offset** (Vá»‹ trÃ­)

Vá»‹ trÃ­ cá»§a message trong partition:

```
Partition 0:
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”
â”‚ 0  â”‚ 1  â”‚ 2  â”‚ 3  â”‚ 4  â”‚ 5  â”‚ 6  â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜
                     â†‘
              Current offset = 3
              (Consumer Ä‘Ã£ Ä‘á»c Ä‘áº¿n message 3)
```

**Offset management:**

```python
# Auto commit (máº·c Ä‘á»‹nh)
consumer = KafkaConsumer(
    'orders',
    enable_auto_commit=True,
    auto_commit_interval_ms=5000  # Commit má»—i 5 giÃ¢y
)

# Manual commit (an toÃ n hÆ¡n)
consumer = KafkaConsumer(
    'orders',
    enable_auto_commit=False
)

for message in consumer:
    try:
        process_order(message.value)
        consumer.commit()  # Chá»‰ commit khi xá»­ lÃ½ thÃ nh cÃ´ng
    except Exception as e:
        print(f"Error: {e}")
        # KhÃ´ng commit â†’ Sáº½ xá»­ lÃ½ láº¡i message nÃ y
```

---

### 7ï¸âƒ£ **Replication** (Sao lÆ°u)

Má»—i partition cÃ³ nhiá»u replicas Ä‘á»ƒ Ä‘áº£m báº£o availability:

```
Topic "orders" - Partition 0 (Replication factor = 3)

Broker 1: [Leader] â”€â”€â”
Broker 2: [Replica]  â”œâ”€ Sync
Broker 3: [Replica] â”€â”€â”˜

Producer â†’ Ghi vÃ o Leader
Consumer â†’ Äá»c tá»« Leader
Náº¿u Leader cháº¿t â†’ 1 Replica trá»Ÿ thÃ nh Leader má»›i
```

---

### 8ï¸âƒ£ **Retention** (LÆ°u trá»¯)

Kafka giá»¯ messages trong má»™t khoáº£ng thá»i gian:

```bash
# Giá»¯ 7 ngÃ y
log.retention.hours=168

# Hoáº·c giá»¯ tá»‘i Ä‘a 1GB
log.retention.bytes=1073741824

# Consumer cÃ³ thá»ƒ replay messages cÅ©
consumer = KafkaConsumer(
    'orders',
    auto_offset_reset='earliest'  # Äá»c tá»« message cÅ© nháº¥t cÃ²n lÆ°u
)
```

---

## 2.5. CÃ¡c use case thá»±c táº¿

### ğŸ“Œ Use Case 1: **Event-Driven Architecture**

Há»‡ thá»‘ng e-commerce vá»›i nhiá»u services:

```python
# ===== Order Service =====
def create_order(order_data):
    # LÆ°u vÃ o database
    order_id = db.save_order(order_data)
    
    # Publish event
    event = {
        "event_type": "ORDER_CREATED",
        "order_id": order_id,
        "user_id": order_data['user_id'],
        "amount": order_data['amount'],
        "items": order_data['items'],
        "timestamp": datetime.now().isoformat()
    }
    
    producer.send('order.events', value=event)
    return order_id

# ===== Email Service (Consumer) =====
def email_consumer():
    consumer = KafkaConsumer('order.events', group_id='email-service')
    
    for message in consumer:
        event = message.value
        
        if event['event_type'] == 'ORDER_CREATED':
            send_order_confirmation_email(
                user_id=event['user_id'],
                order_id=event['order_id']
            )

# ===== Inventory Service (Consumer) =====
def inventory_consumer():
    consumer = KafkaConsumer('order.events', group_id='inventory-service')
    
    for message in consumer:
        event = message.value
        
        if event['event_type'] == 'ORDER_CREATED':
            reserve_inventory(event['items'])

# ===== Analytics Service (Consumer) =====
def analytics_consumer():
    consumer = KafkaConsumer('order.events', group_id='analytics-service')
    
    for message in consumer:
        event = message.value
        
        if event['event_type'] == 'ORDER_CREATED':
            track_revenue(event['amount'])
            update_sales_report(event)
```

**Lá»£i Ã­ch:**
- Services Ä‘á»™c láº­p hoÃ n toÃ n
- Dá»… thÃªm service má»›i (chá»‰ cáº§n thÃªm consumer)
- Fault tolerant
- CÃ³ thá»ƒ replay events

---

### ğŸ“Œ Use Case 2: **Log Aggregation**

Thu tháº­p logs tá»« nhiá»u servers:

```python
# ===== Application Servers (Producers) =====
# Server 1, 2, 3, ... táº¥t cáº£ gá»­i logs vÃ o Kafka

import logging
from kafka.logging.handlers import KafkaHandler

logger = logging.getLogger('my_app')
handler = KafkaHandler(
    hosts='localhost:9092',
    topic='application.logs'
)
logger.addHandler(handler)

# Logs tá»± Ä‘á»™ng gá»­i vÃ o Kafka
logger.info("User logged in", extra={"user_id": "123"})
logger.error("Payment failed", extra={"order_id": "456"})

# ===== Log Processor (Consumer) =====
def log_processor():
    consumer = KafkaConsumer('application.logs', group_id='log-processor')
    
    for message in consumer:
        log = message.value
        
        # LÆ°u vÃ o Elasticsearch
        elasticsearch.index('logs', log)
        
        # Alert náº¿u error
        if log['level'] == 'ERROR':
            send_alert_to_slack(log['message'])
```

---

### ğŸ“Œ Use Case 3: **Real-time Analytics & Metrics**

Theo dÃµi metrics real-time:

```python
# ===== Application (Producer) =====
def track_event(event_type, properties):
    event = {
        "event_type": event_type,
        "properties": properties,
        "timestamp": time.time()
    }
    producer.send('analytics.events', value=event)

# VÃ­ dá»¥ tracking
track_event("page_view", {"page": "/products", "user_id": "123"})
track_event("button_click", {"button": "buy_now", "product_id": "456"})

# ===== Real-time Dashboard (Consumer) =====
from collections import defaultdict
import time

def realtime_dashboard():
    consumer = KafkaConsumer('analytics.events', group_id='dashboard')
    
    # Metrics trong 1 phÃºt
    metrics = defaultdict(int)
    window_start = time.time()
    
    for message in consumer:
        event = message.value
        
        # Reset window má»—i phÃºt
        if time.time() - window_start > 60:
            print(f"Last minute metrics: {dict(metrics)}")
            metrics.clear()
            window_start = time.time()
        
        # Äáº¿m events
        metrics[event['event_type']] += 1
        
        # Update dashboard real-time
        update_dashboard(metrics)
```

---

### ğŸ“Œ Use Case 4: **Stream Processing**

Xá»­ lÃ½ luá»“ng dá»¯ liá»‡u real-time:

```python
# ===== Fraud Detection System =====
from kafka import KafkaConsumer, KafkaProducer
import json

def fraud_detector():
    consumer = KafkaConsumer('transactions', group_id='fraud-detector')
    alert_producer = KafkaProducer()
    
    # Theo dÃµi transactions cá»§a user trong 5 phÃºt
    user_transactions = defaultdict(list)
    
    for message in consumer:
        transaction = message.value
        user_id = transaction['user_id']
        
        # ThÃªm vÃ o history
        user_transactions[user_id].append(transaction)
        
        # Giá»¯ chá»‰ 5 phÃºt gáº§n nháº¥t
        cutoff_time = time.time() - 300
        user_transactions[user_id] = [
            t for t in user_transactions[user_id]
            if t['timestamp'] > cutoff_time
        ]
        
        # Kiá»ƒm tra fraud
        if is_suspicious(user_transactions[user_id]):
            alert = {
                "user_id": user_id,
                "reason": "Multiple transactions in short time",
                "transactions": user_transactions[user_id]
            }
            alert_producer.send('fraud.alerts', value=alert)
            
            # Block user táº¡m thá»i
            block_user(user_id)

def is_suspicious(transactions):
    # QuÃ¡ 5 transactions trong 5 phÃºt
    if len(transactions) > 5:
        return True
    
    # Tá»•ng amount quÃ¡ lá»›n
    total = sum(t['amount'] for t in transactions)
    if total > 10000:
        return True
    
    return False
```

---

### ğŸ“Œ Use Case 5: **Microservices Communication**

Services giao tiáº¿p qua Kafka:

```python
# ===== User Service =====
def register_user(user_data):
    # LÆ°u user
    user_id = db.create_user(user_data)
    
    # Publish event
    event = {
        "event_type": "USER_REGISTERED",
        "user_id": user_id,
        "email": user_data['email'],
        "name": user_data['name']
    }
    producer.send('user.events', value=event)
    
    return user_id

# ===== Email Service (láº¯ng nghe USER_REGISTERED) =====
def email_service():
    consumer = KafkaConsumer('user.events', group_id='email-service')
    
    for message in consumer:
        event = message.value
        
        if event['event_type'] == 'USER_REGISTERED':
            send_welcome_email(event['email'], event['name'])

# ===== Recommendation Service (láº¯ng nghe USER_REGISTERED) =====
def recommendation_service():
    consumer = KafkaConsumer('user.events', group_id='recommendation-service')
    
    for message in consumer:
        event = message.value
        
        if event['event_type'] == 'USER_REGISTERED':
            initialize_recommendations(event['user_id'])

# ===== Analytics Service (láº¯ng nghe USER_REGISTERED) =====
def analytics_service():
    consumer = KafkaConsumer('user.events', group_id='analytics-service')
    
    for message in consumer:
        event = message.value
        
        if event['event_type'] == 'USER_REGISTERED':
            track_new_user(event)
            update_growth_metrics()
```

---

## 2.6. Kafka nÃ¢ng cao

### ğŸ¯ **Kafka Streams** (Stream processing framework)

Xá»­ lÃ½ dá»¯ liá»‡u real-time trá»±c tiáº¿p trong Kafka:

```python
from kafka import KafkaConsumer, KafkaProducer
import json

# VÃ­ dá»¥: TÃ­nh tá»•ng doanh thu theo tá»«ng sáº£n pháº©m real-time
def product_revenue_aggregator():
    consumer = KafkaConsumer('orders', group_id='revenue-aggregator')
    producer = KafkaProducer()
    
    # State: Tá»•ng revenue cá»§a má»—i product
    product_revenue = defaultdict(float)
    
    for message in consumer:
        order = message.value
        
        # Cáº­p nháº­t revenue
        for item in order['items']:
            product_id = item['product_id']
            revenue = item['price'] * item['quantity']
            product_revenue[product_id] += revenue
        
        # Publish káº¿t quáº£ aggregate
        for product_id, total_revenue in product_revenue.items():
            result = {
                "product_id": product_id,
                "total_revenue": total_revenue,
                "timestamp": time.time()
            }
            producer.send('product.revenue', value=result)
```

---

### ğŸ”„ **Exactly-Once Semantics**

Äáº£m báº£o message Ä‘Æ°á»£c xá»­ lÃ½ Ä‘Ãºng 1 láº§n:

```python
from kafka import KafkaConsumer, KafkaProducer

# Producer vá»›i idempotence
producer = KafkaProducer(
    enable_idempotence=True,  # TrÃ¡nh duplicate
    transactional_id='my-transactional-id'
)

# Consumer vá»›i transaction
consumer = KafkaConsumer(
    'orders',
    group_id='payment-processor',
    enable_auto_commit=False,
    isolation_level='read_committed'  # Chá»‰ Ä‘á»c committed messages
)

# Xá»­ lÃ½ transactional
producer.init_transactions()

for message in consumer:
    try:
        producer.begin_transaction()
        
        # Xá»­ lÃ½ message
        order = message.value
        process_payment(order)
        
        # Gá»­i káº¿t quáº£
        result = {"order_id": order['id'], "status": "paid"}
        producer.send('payment.results', value=result)
        
        # Commit offset vÃ  transaction cÃ¹ng lÃºc
        producer.send_offsets_to_transaction(
            {TopicPartition('orders', message.partition): message.offset + 1},
            consumer.config['group_id']
        )
        
        producer.commit_transaction()
        
    except Exception as e:
        producer.abort_transaction()
        print(f"Transaction aborted: {e}")
```

---

### ğŸ“Š **Kafka Connect** (TÃ­ch há»£p vá»›i há»‡ thá»‘ng khÃ¡c)

Káº¿t ná»‘i Kafka vá»›i databases, S3, Elasticsearch, ...

```yaml
# VÃ­ dá»¥: Sync data tá»« MySQL vÃ o Kafka
# jdbc-source-connector.json
{
  "name": "mysql-source-connector",
  "config": {
    "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
    "connection.url": "jdbc:mysql://localhost:3306/mydb",
    "connection.user": "user",
    "connection.password": "password",
    "table.whitelist": "orders,users",
    "mode": "incrementing",
    "incrementing.column.name": "id",
    "topic.prefix": "mysql-"
  }
}

# Káº¿t quáº£:
# - Table "orders" â†’ Topic "mysql-orders"
# - Table "users" â†’ Topic "mysql-users"
# - Tá»± Ä‘á»™ng sync khi cÃ³ thay Ä‘á»•i
```

---

### âš¡ **Performance Tuning**

```python
# ===== Producer Optimization =====
producer = KafkaProducer(
    # Batching Ä‘á»ƒ giáº£m network calls
    batch_size=32768,  # 32KB
    linger_ms=20,      # Chá» 20ms Ä‘á»ƒ gom batch
    
    # Compression
    compression_type='snappy',  # hoáº·c 'gzip', 'lz4'
    
    # Buffer
    buffer_memory=67108864,  # 64MB
    
    # Acks
    acks='1',  # Chá»‰ chá» leader (nhanh hÆ¡n 'all')
    
    # Retry
    retries=3,
    retry_backoff_ms=100
)

# ===== Consumer Optimization =====
consumer = KafkaConsumer(
    'orders',
    # Fetch size
    fetch_min_bytes=1024,       # 1KB
    fetch_max_wait_ms=500,      # Chá» tá»‘i Ä‘a 500ms
    max_partition_fetch_bytes=1048576,  # 1MB
    
    # Batch processing
    max_poll_records=500,  # Láº¥y tá»‘i Ä‘a 500 messages/láº§n
    
    # Session timeout
    session_timeout_ms=30000,
    heartbeat_interval_ms=3000
)

# Xá»­ lÃ½ batch thay vÃ¬ tá»«ng message
messages = consumer.poll(timeout_ms=1000, max_records=500)
for topic_partition, records in messages.items():
    # Xá»­ lÃ½ hÃ ng loáº¡t
    batch_process(records)
    consumer.commit()
```

---

# Pháº§n 3: á»¨ng Dá»¥ng Thá»±c Táº¿

## 3.1. Kiáº¿n trÃºc á»©ng dá»¥ng demo

ChÃºng ta sáº½ xÃ¢y dá»±ng má»™t **há»‡ thá»‘ng xá»­ lÃ½ Ä‘Æ¡n hÃ ng** Ä‘á»ƒ so sÃ¡nh:

### âŒ **PhiÃªn báº£n 1: KHÃ”NG dÃ¹ng Redis & Kafka**

```
User â†’ API Server â†’ Database
              â†“
        (Cháº­m, coupling, khÃ´ng scale)
```

### âœ… **PhiÃªn báº£n 2: Sá»¬ Dá»¤NG Redis & Kafka**

```
                    â”Œâ”€â†’ Redis (Cache)
User â†’ API Server â”€â”€â”¤
                    â””â”€â†’ Kafka â†’ [Email Service]
                              â†’ [SMS Service]
                              â†’ [Inventory Service]
                              â†’ [Analytics Service]
```

---

## 3.2. So sÃ¡nh hiá»‡u suáº¥t

### ğŸ“Š **Metrics Ä‘á»ƒ Ä‘o**

1. **Response Time**: Thá»i gian user chá»
2. **Throughput**: Sá»‘ requests xá»­ lÃ½/giÃ¢y
3. **Database Load**: Sá»‘ queries Ä‘áº¿n database
4. **Failure Rate**: Tá»· lá»‡ lá»—i
5. **Scalability**: Kháº£ nÄƒng má»Ÿ rá»™ng

### ğŸ“ˆ **Káº¿t quáº£ dá»± kiáº¿n**

| Metric | KhÃ´ng Redis/Kafka | CÃ³ Redis/Kafka | Cáº£i thiá»‡n |
|--------|-------------------|----------------|-----------|
| Response Time | 800ms | 55ms | **14.5x nhanh hÆ¡n** |
| Throughput | 100 req/s | 5000 req/s | **50x nhiá»u hÆ¡n** |
| DB Queries | 100% requests | 1% requests | **99% giáº£m** |
| Failure Rate | 5% | 0.1% | **50x Ã­t hÆ¡n** |
| Max Users | 1,000 | 100,000 | **100x scale** |

---

# Pháº§n 4: Best Practices

## ğŸ¯ **Redis Best Practices**

### 1. **Naming Convention**

```python
# âœ… Tá»T - RÃµ rÃ ng, cÃ³ cáº¥u trÃºc
"user:1001:profile"
"product:5432:details"
"session:abc123"
"cache:homepage:en"

# âŒ KHÃ”NG Tá»T - KhÃ³ hiá»ƒu, khÃ´ng cáº¥u trÃºc
"u1001"
"p5432"
"s1"
```

### 2. **Set Expiration**

```python
# âœ… LUÃ”N set expiration Ä‘á»ƒ trÃ¡nh memory leak
redis.setex("session:abc123", 3600, session_data)  # Expire sau 1 giá»

# âŒ KhÃ´ng set expiration = Tá»‘n memory vÃ´ háº¡n
redis.set("session:abc123", session_data)  # BAD!
```

### 3. **Use Pipeline cho nhiá»u commands**

```python
# âŒ CHáº¬M - 3 network round-trips
redis.set("key1", "value1")
redis.set("key2", "value2")
redis.set("key3", "value3")

# âœ… NHANH - 1 network round-trip
pipe = redis.pipeline()
pipe.set("key1", "value1")
pipe.set("key2", "value2")
pipe.set("key3", "value3")
pipe.execute()
```

### 4. **Monitor Memory**

```python
# Kiá»ƒm tra memory usage
info = redis.info('memory')
print(f"Used memory: {info['used_memory_human']}")
print(f"Max memory: {info['maxmemory_human']}")

# Set max memory policy
redis.config_set('maxmemory-policy', 'allkeys-lru')  # XÃ³a keys Ã­t dÃ¹ng nháº¥t
```

### 5. **Avoid Large Keys**

```python
# âŒ KHÃ”NG Tá»T - Key quÃ¡ lá»›n
large_list = list(range(1000000))
redis.set("big_data", json.dumps(large_list))  # 10MB+

# âœ… Tá»T - Chia nhá»
for i in range(10):
    chunk = large_list[i*100000:(i+1)*100000]
    redis.set(f"data:chunk:{i}", json.dumps(chunk))
```

---

## ğŸ¯ **Kafka Best Practices**

### 1. **Topic Naming Convention**

```python
# âœ… Tá»T
"user.registered"
"order.created"
"payment.completed"
"analytics.pageview"

# âŒ KHÃ”NG Tá»T
"users"
"orders"
"data"
```

### 2. **Choose Right Partition Key**

```python
# âœ… Tá»T - Äáº£m báº£o ordering cho cÃ¹ng user
producer.send('orders',
              key=str(user_id).encode(),
              value=order_data)

# âŒ KHÃ”NG Tá»T - KhÃ´ng cÃ³ key = random partition = máº¥t ordering
producer.send('orders', value=order_data)
```

### 3. **Handle Failures**

```python
# âœ… Tá»T - Xá»­ lÃ½ lá»—i vÃ  retry
def process_message(message):
    max_retries = 3
    for attempt in range(max_retries):
        try:
            # Xá»­ lÃ½ message
            result = do_something(message.value)
            
            # Commit offset khi thÃ nh cÃ´ng
            consumer.commit()
            return result
            
        except Exception as e:
            if attempt == max_retries - 1:
                # Gá»­i vÃ o Dead Letter Queue
                producer.send('failed.messages', value={
                    "original_message": message.value,
                    "error": str(e),
                    "timestamp": time.time()
                })
            else:
                time.sleep(2 ** attempt)  # Exponential backoff
```

### 4. **Monitor Lag**

```python
# Consumer lag = Sá»‘ messages chÆ°a xá»­ lÃ½
# Náº¿u lag tÄƒng â†’ Consumer xá»­ lÃ½ khÃ´ng ká»‹p â†’ Cáº§n scale

# Monitoring
def check_consumer_lag():
    admin_client = KafkaAdminClient()
    consumer_groups = admin_client.describe_consumer_groups(['my-group'])
    
    for group in consumer_groups:
        for member in group.members:
            lag = member.lag
            if lag > 10000:
                alert(f"High lag: {lag} messages")
```

### 5. **Right Number of Partitions**

```python
# CÃ´ng thá»©c:
# Partitions = max(
#     Target Throughput / Producer Throughput per Partition,
#     Target Throughput / Consumer Throughput per Partition
# )

# VÃ­ dá»¥:
# - Target: 100MB/s
# - Producer: 10MB/s/partition
# - Consumer: 20MB/s/partition
# â†’ Partitions = max(100/10, 100/20) = 10 partitions
```

---

## ğŸ”’ **Security Best Practices**

### Redis:

```bash
# redis.conf
# 1. Äá»•i port máº·c Ä‘á»‹nh
port 6380

# 2. Set password
requirepass your_strong_password

# 3. Disable dangerous commands
rename-command FLUSHDB ""
rename-command FLUSHALL ""
rename-command CONFIG ""

# 4. Bind to specific IP
bind 127.0.0.1 192.168.1.100
```

### Kafka:

```properties
# server.properties
# 1. Enable SSL
listeners=SSL://0.0.0.0:9093
ssl.keystore.location=/var/private/ssl/kafka.server.keystore.jks
ssl.keystore.password=your_password

# 2. Enable SASL authentication
sasl.enabled.mechanisms=PLAIN
sasl.mechanism.inter.broker.protocol=PLAIN

# 3. Enable ACLs
authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer
super.users=User:admin
```

---

## ğŸ“š TÃ i NguyÃªn Há»c Táº­p

### Redis:
- ğŸ“– [Redis Documentation](https://redis.io/documentation)
- ğŸ“º [Redis University](https://university.redis.com/)
- ğŸ“ [Redis Best Practices](https://redis.io/topics/best-practices)

### Kafka:
- ğŸ“– [Kafka Documentation](https://kafka.apache.org/documentation/)
- ğŸ“º [Kafka Tutorials](https://kafka-tutorials.confluent.io/)
- ğŸ“ [Confluent Blog](https://www.confluent.io/blog/)

---

## ğŸ“ BÃ i Táº­p Thá»±c HÃ nh

### Level 1: Beginner
1. CÃ i Ä‘áº·t Redis vÃ  Kafka
2. Thá»±c hiá»‡n cÃ¡c lá»‡nh cÆ¡ báº£n
3. Viáº¿t Producer vÃ  Consumer Ä‘Æ¡n giáº£n

### Level 2: Intermediate
1. XÃ¢y dá»±ng cache layer vá»›i Redis
2. Implement event-driven architecture vá»›i Kafka
3. Xá»­ lÃ½ failover vÃ  retry

### Level 3: Advanced
1. Setup Redis Cluster
2. Implement Kafka Streams
3. Optimize performance vÃ  monitoring

---

## ğŸ“ Káº¿t Luáº­n

### Khi nÃ o dÃ¹ng Redis?
- âœ… Cáº§n tá»‘c Ä‘á»™ Ä‘á»c/ghi cá»±c nhanh
- âœ… Cache dá»¯ liá»‡u thÆ°á»ng xuyÃªn truy cáº­p
- âœ… Session management
- âœ… Real-time analytics
- âœ… Rate limiting, leaderboard

### Khi nÃ o dÃ¹ng Kafka?
- âœ… Event-driven architecture
- âœ… Microservices communication
- âœ… Log aggregation
- âœ… Stream processing
- âœ… Real-time data pipeline

### Khi nÃ o dÃ¹ng Cáº¢ HAI?
- âœ… Há»‡ thá»‘ng lá»›n, phá»©c táº¡p
- âœ… Cáº§n cáº£ cache LáºªN message queue
- âœ… Real-time + High performance
- âœ… Scalability cao

---

**ChÃºc báº¡n há»c tá»‘t! ğŸš€**

*Náº¿u cÃ³ cÃ¢u há»i gÃ¬, Ä‘á»«ng ngáº¡i há»i tháº§y nhÃ©! ğŸ˜Š*

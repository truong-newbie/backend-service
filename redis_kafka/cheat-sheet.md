# ðŸ“ Redis & Kafka Cheat Sheet - Tra Cá»©u Nhanh

## ðŸ”´ Redis Commands

### String Operations
```bash
# Set/Get
SET key value
GET key
SETEX key seconds value          # Set vá»›i expire
SETNX key value                  # Set náº¿u key chÆ°a tá»“n táº¡i
DEL key                          # XÃ³a key

# Increment/Decrement
INCR key                         # TÄƒng 1
INCRBY key amount                # TÄƒng amount
DECR key                         # Giáº£m 1
DECRBY key amount                # Giáº£m amount
```

### Hash Operations
```bash
# Set/Get
HSET key field value
HGET key field
HGETALL key                      # Láº¥y táº¥t cáº£
HMSET key field1 value1 field2 value2
HDEL key field

# Operations
HINCRBY key field amount         # TÄƒng field
HEXISTS key field                # Check field tá»“n táº¡i
HLEN key                         # Sá»‘ lÆ°á»£ng fields
```

### List Operations
```bash
# Push/Pop
LPUSH key value                  # ThÃªm Ä‘áº§u list
RPUSH key value                  # ThÃªm cuá»‘i list
LPOP key                         # Láº¥y Ä‘áº§u list
RPOP key                         # Láº¥y cuá»‘i list

# Range
LRANGE key start stop            # Láº¥y range
LLEN key                         # Äá»™ dÃ i list
LTRIM key start stop             # Giá»¯ chá»‰ range
```

### Set Operations
```bash
# Add/Remove
SADD key member                  # ThÃªm member
SREM key member                  # XÃ³a member
SMEMBERS key                     # Láº¥y táº¥t cáº£ members

# Operations
SISMEMBER key member             # Check member tá»“n táº¡i
SCARD key                        # Sá»‘ lÆ°á»£ng members
SINTER key1 key2                 # Intersection
SUNION key1 key2                 # Union
SDIFF key1 key2                  # Difference
```

### Sorted Set Operations
```bash
# Add/Remove
ZADD key score member            # ThÃªm vá»›i score
ZREM key member                  # XÃ³a member

# Range
ZRANGE key start stop [WITHSCORES]        # TÄƒng dáº§n
ZREVRANGE key start stop [WITHSCORES]     # Giáº£m dáº§n
ZRANGEBYSCORE key min max                 # Range theo score

# Operations
ZSCORE key member                # Láº¥y score
ZRANK key member                 # Láº¥y rank (tÄƒng dáº§n)
ZREVRANK key member              # Láº¥y rank (giáº£m dáº§n)
ZINCRBY key amount member        # TÄƒng score
```

### Key Management
```bash
# Expiration
EXPIRE key seconds               # Set expire
TTL key                          # Xem thá»i gian cÃ²n láº¡i
PERSIST key                      # XÃ³a expire

# Info
EXISTS key                       # Check key tá»“n táº¡i
TYPE key                         # Kiá»ƒu dá»¯ liá»‡u
KEYS pattern                     # TÃ¬m keys (KHÃ”NG dÃ¹ng production!)
SCAN cursor [MATCH pattern]      # TÃ¬m keys an toÃ n

# Rename
RENAME key newkey
```

### Pub/Sub
```bash
# Publisher
PUBLISH channel message

# Subscriber
SUBSCRIBE channel
UNSUBSCRIBE channel
PSUBSCRIBE pattern               # Subscribe theo pattern
```

### Transactions
```bash
MULTI                            # Báº¯t Ä‘áº§u transaction
... commands ...
EXEC                             # Thá»±c thi
DISCARD                          # Há»§y transaction
WATCH key                        # Watch key
```

---

## ðŸŸ¢ Kafka Commands

### Topic Management
```bash
# Create topic
kafka-topics --create \
  --bootstrap-server localhost:9092 \
  --topic my-topic \
  --partitions 3 \
  --replication-factor 2

# List topics
kafka-topics --list \
  --bootstrap-server localhost:9092

# Describe topic
kafka-topics --describe \
  --bootstrap-server localhost:9092 \
  --topic my-topic

# Delete topic
kafka-topics --delete \
  --bootstrap-server localhost:9092 \
  --topic my-topic
```

### Producer Commands
```bash
# Console producer
kafka-console-producer \
  --bootstrap-server localhost:9092 \
  --topic my-topic

# Producer with key
kafka-console-producer \
  --bootstrap-server localhost:9092 \
  --topic my-topic \
  --property "parse.key=true" \
  --property "key.separator=:"
```

### Consumer Commands
```bash
# Console consumer
kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic my-topic

# From beginning
kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic my-topic \
  --from-beginning

# With group
kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic my-topic \
  --group my-group

# With key
kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic my-topic \
  --property print.key=true \
  --property key.separator=":"
```

### Consumer Group Management
```bash
# List groups
kafka-consumer-groups --list \
  --bootstrap-server localhost:9092

# Describe group
kafka-consumer-groups --describe \
  --bootstrap-server localhost:9092 \
  --group my-group

# Reset offset
kafka-consumer-groups --reset-offsets \
  --bootstrap-server localhost:9092 \
  --group my-group \
  --topic my-topic \
  --to-earliest \
  --execute
```

### Performance Testing
```bash
# Producer performance test
kafka-producer-perf-test \
  --topic my-topic \
  --num-records 1000000 \
  --record-size 1024 \
  --throughput -1 \
  --producer-props bootstrap.servers=localhost:9092

# Consumer performance test
kafka-consumer-perf-test \
  --topic my-topic \
  --bootstrap-server localhost:9092 \
  --messages 1000000
```

---

## ðŸ Python Code Snippets

### Redis - Python
```python
import redis

# Connect
r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)

# String
r.set('name', 'John')
name = r.get('name')

# Hash
r.hset('user:1', mapping={'name': 'John', 'age': 25})
user = r.hgetall('user:1')

# List
r.rpush('queue', 'task1', 'task2')
task = r.lpop('queue')

# Set
r.sadd('tags', 'python', 'redis', 'kafka')
tags = r.smembers('tags')

# Sorted Set
r.zadd('leaderboard', {'player1': 100, 'player2': 200})
top = r.zrevrange('leaderboard', 0, 9, withscores=True)

# Pipeline (batch)
pipe = r.pipeline()
pipe.set('key1', 'value1')
pipe.set('key2', 'value2')
pipe.execute()

# Pub/Sub
pubsub = r.pubsub()
pubsub.subscribe('news')
for message in pubsub.listen():
    print(message)
```

### Kafka - Python Producer
```python
from kafka import KafkaProducer
import json

# Create producer
producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    value_serializer=lambda v: json.dumps(v).encode('utf-8'),
    acks='all',
    retries=3
)

# Send message
data = {'user_id': 123, 'action': 'login'}
future = producer.send('events', value=data)

# Send with key
producer.send('events', key=b'user_123', value=data)

# Wait for confirmation
record = future.get(timeout=10)
print(f"Sent to partition {record.partition} at offset {record.offset}")

# Close
producer.close()
```

### Kafka - Python Consumer
```python
from kafka import KafkaConsumer
import json

# Create consumer
consumer = KafkaConsumer(
    'events',
    bootstrap_servers='localhost:9092',
    group_id='my-group',
    value_deserializer=lambda m: json.loads(m.decode('utf-8')),
    auto_offset_reset='earliest',  # 'earliest' hoáº·c 'latest'
    enable_auto_commit=True
)

# Consume messages
for message in consumer:
    print(f"Topic: {message.topic}")
    print(f"Partition: {message.partition}")
    print(f"Offset: {message.offset}")
    print(f"Key: {message.key}")
    print(f"Value: {message.value}")

# Manual commit
consumer = KafkaConsumer(
    'events',
    enable_auto_commit=False
)

for message in consumer:
    process(message.value)
    consumer.commit()  # Commit sau khi xá»­ lÃ½ xong
```

---

## ðŸŽ¯ Common Patterns

### Pattern 1: Cache-Aside
```python
def get_user(user_id):
    # Try cache first
    user = redis.get(f"user:{user_id}")
    if user:
        return json.loads(user)
    
    # Cache miss - get from DB
    user = db.get_user(user_id)
    
    # Save to cache
    redis.setex(f"user:{user_id}", 3600, json.dumps(user))
    
    return user
```

### Pattern 2: Write-Through Cache
```python
def update_user(user_id, data):
    # Update DB
    db.update_user(user_id, data)
    
    # Update cache
    redis.setex(f"user:{user_id}", 3600, json.dumps(data))
```

### Pattern 3: Event Sourcing
```python
# Producer
def create_order(order_data):
    order_id = db.save_order(order_data)
    
    event = {
        'event_type': 'ORDER_CREATED',
        'order_id': order_id,
        'timestamp': time.time(),
        'data': order_data
    }
    
    producer.send('orders', value=event)
    return order_id

# Consumer
consumer = KafkaConsumer('orders')
for message in consumer:
    event = message.value
    
    if event['event_type'] == 'ORDER_CREATED':
        send_confirmation_email(event['data'])
```

### Pattern 4: CQRS (Command Query Responsibility Segregation)
```python
# Write Side (Command)
def create_product(data):
    product_id = db.insert(data)
    
    # Publish event
    producer.send('product.events', {
        'type': 'PRODUCT_CREATED',
        'product_id': product_id,
        'data': data
    })

# Read Side (Query)
consumer = KafkaConsumer('product.events')
for message in consumer:
    event = message.value
    
    # Update read-optimized store (Redis)
    if event['type'] == 'PRODUCT_CREATED':
        redis.hset(f"product:{event['product_id']}", 
                   mapping=event['data'])
```

### Pattern 5: Rate Limiting (Sliding Window)
```python
def is_rate_limited(user_id, limit=10, window=60):
    key = f"rate:{user_id}"
    current = time.time()
    
    # Remove old entries
    redis.zremrangebyscore(key, 0, current - window)
    
    # Count requests in window
    count = redis.zcard(key)
    
    if count >= limit:
        return True
    
    # Add current request
    redis.zadd(key, {str(current): current})
    redis.expire(key, window)
    
    return False
```

---

## ðŸ”§ Configuration Templates

### Redis Configuration (redis.conf)
```ini
# Network
bind 127.0.0.1
port 6379
protected-mode yes

# Memory
maxmemory 2gb
maxmemory-policy allkeys-lru

# Persistence
save 900 1
save 300 10
save 60 10000
appendonly yes
appendfsync everysec

# Security
requirepass your_password_here

# Performance
tcp-backlog 511
timeout 300
```

### Kafka Producer Config
```python
producer_config = {
    'bootstrap.servers': 'localhost:9092',
    'acks': 'all',                    # 0, 1, or all
    'retries': 3,
    'max.in.flight.requests.per.connection': 5,
    'compression.type': 'snappy',     # none, gzip, snappy, lz4
    'batch.size': 16384,
    'linger.ms': 10,
    'buffer.memory': 33554432,
}
```

### Kafka Consumer Config
```python
consumer_config = {
    'bootstrap.servers': 'localhost:9092',
    'group.id': 'my-group',
    'auto.offset.reset': 'earliest',  # earliest or latest
    'enable.auto.commit': True,
    'auto.commit.interval.ms': 5000,
    'max.poll.records': 500,
    'session.timeout.ms': 30000,
    'heartbeat.interval.ms': 3000,
}
```

---

## ðŸš¨ Common Mistakes & Solutions

### Redis Mistakes

#### âŒ Mistake 1: KhÃ´ng set expiration
```python
# BAD
redis.set('session:abc', data)

# GOOD
redis.setex('session:abc', 3600, data)
```

#### âŒ Mistake 2: DÃ¹ng KEYS trong production
```python
# BAD - Block Redis
keys = redis.keys('user:*')

# GOOD - Non-blocking
cursor = 0
while True:
    cursor, keys = redis.scan(cursor, match='user:*', count=100)
    # Process keys
    if cursor == 0:
        break
```

#### âŒ Mistake 3: LÆ°u data quÃ¡ lá»›n
```python
# BAD
redis.set('big_data', huge_json)  # 10MB+

# GOOD - Chia nhá»
for chunk in chunks(data, 1000):
    redis.set(f'data:{chunk.id}', chunk)
```

### Kafka Mistakes

#### âŒ Mistake 4: KhÃ´ng handle consumer rebalance
```python
# BAD
consumer = KafkaConsumer('topic')
for message in consumer:
    process(message)  # Máº¥t data khi rebalance

# GOOD
consumer = KafkaConsumer(
    'topic',
    enable_auto_commit=False
)
for message in consumer:
    try:
        process(message)
        consumer.commit()
    except Exception:
        # KhÃ´ng commit - sáº½ xá»­ lÃ½ láº¡i
        pass
```

#### âŒ Mistake 5: Partition quÃ¡ Ã­t/nhiá»u
```python
# BAD - 1 partition = khÃ´ng parallel
kafka-topics --create --partitions 1

# BAD - QuÃ¡ nhiá»u partitions = overhead
kafka-topics --create --partitions 1000

# GOOD - Vá»«a pháº£i (thÆ°á»ng 3-10)
kafka-topics --create --partitions 6
```

---

## ðŸ“Š Monitoring Commands

### Redis Monitoring
```bash
# Real-time monitoring
redis-cli monitor

# Stats
redis-cli info
redis-cli info stats
redis-cli info memory

# Slow queries
redis-cli slowlog get 10

# Client list
redis-cli client list
```

### Kafka Monitoring
```bash
# Topic lag
kafka-consumer-groups --describe \
  --bootstrap-server localhost:9092 \
  --group my-group

# Broker metrics
kafka-run-class kafka.tools.JmxTool \
  --object-name kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec

# Under-replicated partitions
kafka-topics --describe \
  --bootstrap-server localhost:9092 \
  --under-replicated-partitions
```

---

## ðŸŽ“ Quick Decision Tree

### Khi nÃ o dÃ¹ng Redis?
```
Cáº§n tá»‘c Ä‘á»™ cá»±c nhanh? â”€â”€YESâ”€â”€> Redis
        â”‚
        NO
        â†“
Cáº§n cache data? â”€â”€YESâ”€â”€> Redis
        â”‚
        NO
        â†“
Session storage? â”€â”€YESâ”€â”€> Redis
        â”‚
        NO
        â†“
Real-time leaderboard? â”€â”€YESâ”€â”€> Redis
```

### Khi nÃ o dÃ¹ng Kafka?
```
Event-driven architecture? â”€â”€YESâ”€â”€> Kafka
        â”‚
        NO
        â†“
Async processing? â”€â”€YESâ”€â”€> Kafka
        â”‚
        NO
        â†“
Log aggregation? â”€â”€YESâ”€â”€> Kafka
        â”‚
        NO
        â†“
Microservices messaging? â”€â”€YESâ”€â”€> Kafka
```

---

## ðŸ”— Useful Links

### Redis
- [Redis Documentation](https://redis.io/documentation)
- [Redis Commands](https://redis.io/commands)
- [Try Redis](https://try.redis.io/)

### Kafka
- [Kafka Documentation](https://kafka.apache.org/documentation/)
- [Confluent Kafka](https://docs.confluent.io/)
- [Kafka Tutorials](https://kafka-tutorials.confluent.io/)

---

**ðŸ’¡ Tip:** In cheat sheet nÃ y ra vÃ  Ä‘á»ƒ bÃªn cáº¡nh khi code!
